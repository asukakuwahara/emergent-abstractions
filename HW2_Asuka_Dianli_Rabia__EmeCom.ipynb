{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e582d26181c082",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Homework 2: Emergent communication models\n",
    "\n",
    "This homework assignment is to be completed in groups. It is due on December 18, 2025 (midnight). Please upload *all files you created or modified* to the homework folder of your group in studIP.\n",
    "\n",
    "Group number:\n",
    "\n",
    "Names:\n",
    "\n",
    "*General note: It is permitted to use AI tools for coding. Please refer to the uploaded manual `AI_Tools_Guidelines` for recommended ways how to use AI to advance your studies in a way that supports your learning. That means that you should not be satisfied if an AI tool hands you a working version of your code, but that you should put in effort to understand how exactly the problem is solved. Another note of caution: What might work for large programming languages like Python, does not necessarily work for Stan. Check your code carefully and do NOT blindly trust AI.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56278084c59e494a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Introduction\n",
    "In the past weeks, you learned about classical neural network approaches to modeling language, about Reinforcement learning and how these two can be combined with the aim of modeling communication in the emergent communication modeling framework.\n",
    "The goal of this homework assignment is to learn to apply these models. You will be working with a Github repository of my own research. Learning how to work with this repository will qualify you for conducting your study project or thesis within this project.\n",
    "Below, you receive a sample project and are guided through the steps of conducting it. For all steps, it is expected that you comment your code, explain what you are doing and why, and interpret all your outcomes and results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a829321fa050e8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1) Creating the environment and installing packages (non-graded)\n",
    "This is a prerequisite for the following tasks and is therefore non-graded. Please let me know if you run into problems with installing everything. Follow the installation guide in the [emergent-abstractions repository](https://github.com/kristinakobrock/emergent-abstractions/tree/main) and validate that you can run the training (with a small number of epochs and dataset). Follow the tutorial in the [Tutorial Github repository](https://github.com/kristinakobrock/tutorial-emergent-abstractions/tree/main) to make yourself familiar with the code.\n",
    "\n",
    "For the following tasks, you can either work directly with the main repository (recommended) or work with the tutorial code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4293a1332aa0fc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2) Dataset (18 points)\n",
    "The goal is to implement a hierarchical dataset that follows the following logic from the paper by Hawkins et al., 2018: [Emerging abstractions](https://escholarship.org/uc/item/81s4d7fv)\n",
    "<img src=\"img/hierarchy.png\" width=\"400\"/>\n",
    "It differs from the default dataset because the default dataset includes all combinations of objects without implementing a hierarchy. Consider the following example for the default dataset with 3 attributes and 3 values. Concepts are implemented as tuples of objects and fixed vectors which indicate which attributes are relevant to the concept, e.g. ([[2,1,1],[1,0,1],[1,2,1]],(0,0,1)). Where the fixed vector (0,0,1) indicates that only the first attribute is relevant to the concept. [2,1,1],[1,0,1],[1,2,1] are possible objects that satisfy this constraint. You can think of the objects as being a small red circle, a large blue circle and a large green circle - all belonging to the concept 'circle'. Now, in the default dataset, all combinations of attributes are possible, i.e. the higher-level concept don't need to be circles, but could also be the color dimension or the size dimension by specifying the fixed vector as (1,0,0) or (0,1,0). While this certainly makes a lot of sense for compositional features as the ones used here, natural concepts are structured in taxonomic hierarchies (e.g., dalmatian, dog, animal). The goal here is to implement a hierarchical dataset.\n",
    "\n",
    "### 2.1) Adapting the dataset (12 points)\n",
    "Adapt the function `get_fixed_vectors` in `dataset.py` to account for hierarchically structured concepts.\n",
    "Your solution should work for varying dataset sizes and be neatly integrated with the currently available code. Specify a folder where the results for this experiment should be saved to.\n",
    "\n",
    "### 2.2) Creating the datasets (6 points)\n",
    "Use the script `pickle_ds.py` to construct 3 datasets with the following attribute-value combinations D(attributes, values):\n",
    "\n",
    "Use this command to run the script and generate the data results:\n",
    "\n",
    "D(3,4) means: \n",
    "3 = Number of Attributes = Types of features\n",
    "4 = Number of Values = Options per feature\n",
    "\n",
    "- D(3,4)\n",
    "  `python pickle_ds.py --dimensions 4 4 4 --scaling_factor 10 --hierarchical True`\\\n",
    "  output: \n",
    "  ```\n",
    "  Found 3 unique fixed vectors:\n",
    "   (0, 0, 1)\n",
    "   (0, 1, 1)\n",
    "   (1, 1, 1)\n",
    "  Number of concepts: 84\n",
    "  ```\n",
    "- D(4,4)\n",
    "  `python pickle_ds.py --dimensions 4 4 4 4 --scaling_factor 10 --hierarchical True`\\\n",
    "    output: \n",
    "    ```\n",
    "  Found 4 unique fixed vectors:\n",
    "   (0, 0, 0, 1)\n",
    "   (0, 0, 1, 1)\n",
    "   (0, 1, 1, 1)\n",
    "   (1, 1, 1, 1)\n",
    "  Number of concepts: 340\n",
    "  ```\n",
    "- D(3,8)\n",
    "  `python pickle_ds.py --dimensions 8 8 8 --scaling_factor 10 --hierarchical True`\\\n",
    "    output: \n",
    "    ```\n",
    "    Found 3 unique fixed vectors:\n",
    "    (0, 0, 1)\n",
    "    (0, 1, 1)\n",
    "    (1, 1, 1)\n",
    "    Number of concepts: 584\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d965b14f7ecd8a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3) Model simulations (12 points)\n",
    "Think about which conditions (i.e., settings) you need to compare to discuss whether the implementation of the hierarchical dataset has any measurable impact on the emerging language. Specify which conditions you will compare and why and state your hypotheses. Simulate five runs for each of the datasets and each of the conditions. For hyperparameters, it is recommended to either stick to hyperparameters that were used before (check publications or default settings) or do a grid search over hyperparameters if you expect different hyperparameters to work better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657fe286526d64fb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4) Analysis (50 points)\n",
    "### 4.1) Model performance (10 points)\n",
    "Assess the model performance for each dataset and condition.\n",
    "### 4.2) Quantitative analysis (25 points)\n",
    "Analyze the emerging languages in your conditions with metrics as you see fit. You can check out the emergent-abstractions github repository for inspiration. Your analysis should at least include the following metrics:\n",
    "- NMI\n",
    "- message length\n",
    "\n",
    "Choose two additional metrics or types of quantitative analysis and interpret all your results.\n",
    "\n",
    "### 4.3) Qualitative analysis (15 points)\n",
    "Look at the emerging messages and concepts from the interaction files (see tutorial). Assess the differences between simulations with and without the hierarchical datasets. Can you make out specific test cases where the results (should) differ? Come up with a good way to visualize your results, either as a tidy table, or a visual representation, for example with clustering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde8b2a10ffedb49",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5) Discussion (20 points)\n",
    "Summarize your results and discuss whether the hierarchical dataset implementation resulted in measurable differences in the emerging language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d735a4d728b20",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Bonus task (15 bonus points)\n",
    "You receive 15 bonus points for trying to replicate the above-mentioned study by [Hawkins et al. (2018)](https://escholarship.org/uc/item/81s4d7fv) with a hierarchical dataset. This can be done in the scope of the assignments above. Suitable parameters for creating datasets with different context granularities are implemented in the main Github repository and can be used for this task. Make sure to present an analysis and discussion linking the simulation results to results from the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb793360a24288",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## X) Reflection (no points, but mandatory)\n",
    "\n",
    "Reflect on your group work. What went well? What did not go well?\n",
    "\n",
    "Please note down the group members' team roles anonymously and reflect on how you filled this role."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
